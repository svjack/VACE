<p align="center">

<h1 align="center">VACE: All-in-One Video Creation and Editing</h1>
<p align="center">
    <strong>Zeyinzi Jiang<sup>*</sup></strong>
    Â·
    <strong>Zhen Han<sup>*</sup></strong>
    Â·
    <strong>Chaojie Mao<sup>*&dagger;</sup></strong>
    Â·
    <strong>Jingfeng Zhang</strong>
    Â·
    <strong>Yulin Pan</strong>
    Â·
    <strong>Yu Liu</strong>
    <br>
    <b>Tongyi Lab - <a href="https://github.com/Wan-Video/Wan2.1"><img src='https://ali-vilab.github.io/VACE-Page/assets/logos/wan_logo.png' alt='wan_logo' style='margin-bottom: -4px; height: 20px;'></a> </b>
    <br>
    <br>
        <a href="https://arxiv.org/abs/2503.07598"><img src='https://img.shields.io/badge/VACE-arXiv-red' alt='Paper PDF'></a>
        <a href="https://ali-vilab.github.io/VACE-Page/"><img src='https://img.shields.io/badge/VACE-Project_Page-green' alt='Project Page'></a>
        <a href="https://huggingface.co/collections/ali-vilab/vace-67eca186ff3e3564726aff38"><img src='https://img.shields.io/badge/VACE-HuggingFace_Model-yellow'></a>
        <a href="https://modelscope.cn/collections/VACE-8fa5fcfd386e43"><img src='https://img.shields.io/badge/VACE-ModelScope_Model-purple'></a>
    <br>
</p>

```bash
git clone https://github.com/ali-vilab/VACE.git && cd VACE
pip install torch==2.5.1 torchvision==0.20.1
pip install -r requirements.txt
pip install wan@git+https://github.com/Wan-Video/Wan2.1  # If you want to use Wan2.1-based VACE.
pip install moviepy==1.0.3
#pip install ltx-video@git+https://github.com/Lightricks/LTX-Video@ltx-video-0.9.1 sentencepiece --no-deps # If you want to use LTX-Video-0.9-based VACE. It may conflict with Wan.

python vace/vace_wan_inference.py --model_name vace-1.3B --src_video <path-to-src-video> --src_mask <path-to-src-mask> --src_ref_images <paths-to-src-ref-images> --prompt <prompt>  # wan

huggingface-cli download  Wan-AI/Wan2.1-VACE-1.3B --local-dir models/Wan2.1-VACE-1.3B --repo-type model

huggingface-cli download ali-vilab/VACE-Benchmark --local-dir benchmarks/VACE-Benchmark --repo-type dataset

python vace/vace_wan_inference.py --model_name vace-1.3B \
 --src_video "benchmarks/VACE-Benchmark/assets/examples/outpainting/src_video.mp4" \
 --src_mask "benchmarks/VACE-Benchmark/assets/examples/outpainting/src_mask.mp4" \
 --prompt "èµ›åšæœ‹å…‹é£æ ¼ï¼Œæ— äººæœºä¿¯ç°è§†è§’ä¸‹çš„ç°ä»£è¥¿å®‰åŸå¢™ï¼Œé•œå¤´ç©¿è¿‡æ°¸å®é—¨æ—¶æ³›èµ·é‡‘è‰²æ¶Ÿæ¼ªï¼ŒåŸå¢™ç –å—åŒ–ä½œæ•°æ®æµé‡ç»„ä¸ºå”ä»£é•¿å®‰åŸã€‚å‘¨å›´çš„è¡—é“ä¸ŠæµåŠ¨çš„äººç¾¤å’Œé£é©°çš„æœºæ¢°äº¤é€šå·¥å…·äº¤ç»‡åœ¨ä¸€èµ·ï¼Œç°ä»£ä¸å¤ä»£çš„äº¤èï¼ŒåŸå¢™ä¸Šçš„ç¯å…‰é—ªçƒï¼Œå½¢æˆæ—¶ç©ºéš§é“çš„æ•ˆæœã€‚å…¨æ¯æŠ•å½±æŠ€æœ¯å±•ç°å†å²å˜è¿ï¼Œç²’å­é‡ç»„ç‰¹æ•ˆç»†è…»é€¼çœŸã€‚å¤§è¿œæ™¯é€æ¸è¿‡æ¸¡åˆ°ç‰¹å†™ï¼Œèšç„¦äºåŸé—¨ç‰¹æ•ˆã€‚"

```

### One Outpainting Demo 
```python
from PIL import Image, ImageDraw
import numpy as np
from moviepy.editor import VideoFileClip, ImageClip
import os

def create_bbox_image(canvas_size, bbox_position, bbox_size):
    """
    åˆ›å»ºå¸¦æœ‰é»‘è‰²bboxçš„ç™½è‰²èƒŒæ™¯å›¾åƒ

    å‚æ•°:
        canvas_size: (width, height) æ•´ä¸ªç”»å¸ƒçš„å¤§å°
        bbox_position: (x, y) bboxå·¦ä¸Šè§’çš„ä½ç½®
        bbox_size: (width, height) bboxçš„å¤§å°
    """
    # åˆ›å»ºç™½è‰²èƒŒæ™¯å›¾åƒ
    image = Image.new('RGB', canvas_size, color='white')
    draw = ImageDraw.Draw(image)

    # ç»˜åˆ¶é»‘è‰²bbox
    x, y = bbox_position
    w, h = bbox_size
    draw.rectangle([x, y, x + w, y + h], fill='black')

    return image

def create_static_video(input_video_path, output_video_path, canvas_size, bbox_position, bbox_size):
    """
    åˆ›å»ºé™æ€è§†é¢‘

    å‚æ•°:
        input_video_path: è¾“å…¥è§†é¢‘è·¯å¾„(ç”¨äºè·å–è§†é¢‘é•¿åº¦)
        output_video_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        canvas_size: (width, height) æ•´ä¸ªç”»å¸ƒçš„å¤§å°
        bbox_position: (x, y) bboxå·¦ä¸Šè§’çš„ä½ç½®
        bbox_size: (width, height) bboxçš„å¤§å°
    """
    # åˆ›å»ºé™æ€å›¾åƒ
    image = create_bbox_image(canvas_size, bbox_position, bbox_size)

    # è·å–è¾“å…¥è§†é¢‘çš„æ—¶é•¿
    with VideoFileClip(input_video_path) as video:
        duration = video.duration

    # å°†å›¾åƒè½¬æ¢ä¸ºè§†é¢‘
    image_array = np.array(image)  # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
    clip = ImageClip(image_array).set_duration(duration)

    # å†™å…¥è¾“å‡ºè§†é¢‘æ–‡ä»¶
    clip.write_videofile(output_video_path, fps=24, codec='libx264', audio=False)

    print(f"é™æ€è§†é¢‘å·²åˆ›å»º: {output_video_path}")

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    input_video = "AnimateDiff_00011-audio.mp4"  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥è§†é¢‘è·¯å¾„
    output_video = "mask.mp4"

    # ç”»å¸ƒå¤§å° (å®½, é«˜)
    canvas_size = (1920, 1080)

    # bboxä½ç½®å’Œå¤§å° (x, y), (å®½, é«˜)
    bbox_position = (850, 100)  # ä¸­å¿ƒä½ç½®ç¤ºä¾‹
    bbox_size = (200, 200)

    # åˆ›å»ºé™æ€è§†é¢‘
    create_static_video(input_video, output_video, canvas_size, bbox_position, bbox_size)


from PIL import Image, ImageDraw, ImageOps
import numpy as np
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip
from moviepy.video.fx.all import resize

def embed_video_in_bbox(input_video_path, output_video_path, canvas_size, bbox_position, bbox_size):
    """
    å°†è¾“å…¥è§†é¢‘åµŒå…¥åˆ°bboxåŒºåŸŸå†…ï¼Œè¾¹ç¼˜ç”¨ç°è‰²å¡«å……

    å‚æ•°:
        input_video_path: è¾“å…¥è§†é¢‘è·¯å¾„
        output_video_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        canvas_size: (width, height) æ•´ä¸ªç”»å¸ƒçš„å¤§å°
        bbox_position: (x, y) bboxå·¦ä¸Šè§’çš„ä½ç½®
        bbox_size: (width, height) bboxçš„å¤§å°
    """
    # åˆ›å»ºç°è‰²èƒŒæ™¯çš„é™æ€å›¾åƒ
    def create_gray_background():
        return Image.new('RGB', canvas_size, color='#808080')  # ç°è‰²èƒŒæ™¯

    # åŠ è½½è¾“å…¥è§†é¢‘
    video_clip = VideoFileClip(input_video_path)

    # è°ƒæ•´è§†é¢‘å¤§å°ä»¥é€‚åº”bbox
    bbox_w, bbox_h = bbox_size
    video_w, video_h = video_clip.size

    # è®¡ç®—ä¿æŒå®½é«˜æ¯”çš„ç¼©æ”¾æ¯”ä¾‹
    ratio = min(bbox_w / video_w, bbox_h / video_h)
    new_w = int(video_w * ratio)
    new_h = int(video_h * ratio)

    # è°ƒæ•´è§†é¢‘å¤§å°
    resized_video = video_clip.fx(resize, width=new_w, height=new_h)

    # è®¡ç®—è§†é¢‘åœ¨bboxä¸­çš„ä½ç½®(å±…ä¸­)
    x, y = bbox_position
    pos_x = x + (bbox_w - new_w) // 2
    pos_y = y + (bbox_h - new_h) // 2

    # åˆ›å»ºç°è‰²èƒŒæ™¯çš„é™æ€è§†é¢‘
    gray_bg = ImageClip(np.array(create_gray_background())).set_duration(video_clip.duration)

    # åˆæˆè§†é¢‘
    final_clip = CompositeVideoClip([
        gray_bg,
        resized_video.set_position((pos_x, pos_y))
    ])

    # å†™å…¥è¾“å‡ºè§†é¢‘æ–‡ä»¶
    final_clip.write_videofile(
        output_video_path,
        fps=video_clip.fps,
        codec='libx264',
        audio_codec='aac',
        threads=4,
        preset='fast',
        bitrate='8000k'
    )

    # å…³é—­è§†é¢‘å‰ªè¾‘
    video_clip.close()
    final_clip.close()

    print(f"æ–°è§†é¢‘å·²ç”Ÿæˆ: {output_video_path}")

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    input_video = "AnimateDiff_00011-audio.mp4"  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥è§†é¢‘è·¯å¾„
    output_video = "AnimateDiff_00011-audio_placed.mp4"

    # ç”»å¸ƒå¤§å° (å®½, é«˜)
    canvas_size = (1920, 1080)

    # bboxä½ç½®å’Œå¤§å° (x, y), (å®½, é«˜)
    bbox_position = (850, 100)  # ä¸­å¿ƒä½ç½®ç¤ºä¾‹
    bbox_size = (200, 200)

    # å°†è§†é¢‘åµŒå…¥bbox
    embed_video_in_bbox(input_video, output_video, canvas_size, bbox_position, bbox_size)

```

```bash
python vace/vace_wan_inference.py --model_name vace-1.3B \
 --src_video "AnimateDiff_00011-audio_placed.mp4" \
 --src_mask "mask.mp4" \
 --prompt "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨è‡ªç„¶é£å…‰ç¾ä¸½çš„æ— äººå®¤å¤–å”±æ­Œã€‚"
```

#### In Loop Outpainting Demo 
```python
from PIL import Image, ImageDraw
import numpy as np
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip
from moviepy.video.fx.all import resize
from pathlib import Path
import os

def get_sorted_files(directory, extension):
    """Return sorted list of files with given extension in directory"""
    try:
        files = sorted(Path(directory).glob(f'*.{extension}'))
        return [str(f) for f in files]
    except Exception as e:
        print(f"Error reading {directory}: {e}")
        return []

def create_bbox_image(canvas_size, bbox_position, bbox_size):
    """
    åˆ›å»ºå¸¦æœ‰é»‘è‰²bboxçš„ç™½è‰²èƒŒæ™¯å›¾åƒ
    """
    image = Image.new('RGB', canvas_size, color='white')
    draw = ImageDraw.Draw(image)
    x, y = bbox_position
    w, h = bbox_size
    draw.rectangle([x, y, x + w, y + h], fill='black')
    return image

def create_static_video(input_video_path, output_video_path, canvas_size, bbox_position, bbox_size):
    """
    åˆ›å»ºé™æ€è§†é¢‘
    """
    image = create_bbox_image(canvas_size, bbox_position, bbox_size)

    with VideoFileClip(input_video_path) as video:
        duration = video.duration

    image_array = np.array(image)
    clip = ImageClip(image_array).set_duration(duration)
    clip.write_videofile(output_video_path, fps=24, codec='libx264', audio=False)

def create_gray_background(canvas_size):
    """åˆ›å»ºç°è‰²èƒŒæ™¯å›¾åƒ"""
    return Image.new('RGB', canvas_size, color='#808080')

def embed_video_in_bbox(input_video_path, output_video_path, canvas_size, bbox_position, bbox_size):
    """
    å°†è¾“å…¥è§†é¢‘åµŒå…¥åˆ°bboxåŒºåŸŸå†…ï¼Œè¾¹ç¼˜ç”¨ç°è‰²å¡«å……
    """
    # åŠ è½½è¾“å…¥è§†é¢‘
    video_clip = VideoFileClip(input_video_path)

    # è°ƒæ•´è§†é¢‘å¤§å°ä»¥é€‚åº”bbox
    bbox_w, bbox_h = bbox_size
    video_w, video_h = video_clip.size

    # è®¡ç®—ä¿æŒå®½é«˜æ¯”çš„ç¼©æ”¾æ¯”ä¾‹
    ratio = min(bbox_w / video_w, bbox_h / video_h)
    new_w = int(video_w * ratio)
    new_h = int(video_h * ratio)

    # è°ƒæ•´è§†é¢‘å¤§å°
    resized_video = video_clip.fx(resize, width=new_w, height=new_h)

    # è®¡ç®—è§†é¢‘åœ¨bboxä¸­çš„ä½ç½®(å±…ä¸­)
    x, y = bbox_position
    pos_x = x + (bbox_w - new_w) // 2
    pos_y = y + (bbox_h - new_h) // 2

    # åˆ›å»ºç°è‰²èƒŒæ™¯çš„é™æ€è§†é¢‘
    gray_bg = ImageClip(np.array(create_gray_background(canvas_size))).set_duration(video_clip.duration)

    # åˆæˆè§†é¢‘
    final_clip = CompositeVideoClip([
        gray_bg,
        resized_video.set_position((pos_x, pos_y))
    ])

    # å†™å…¥è¾“å‡ºè§†é¢‘æ–‡ä»¶
    final_clip.write_videofile(
        output_video_path,
        fps=video_clip.fps,
        codec='libx264',
        audio_codec='aac',
        threads=4,
        preset='fast',
        bitrate='8000k'
    )

    # å…³é—­è§†é¢‘å‰ªè¾‘
    video_clip.close()
    final_clip.close()

def process_all_videos(input_folder, mask_output_folder, placed_output_folder):
    """
    å¤„ç†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰MP4æ–‡ä»¶
    """
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(mask_output_folder, exist_ok=True)
    os.makedirs(placed_output_folder, exist_ok=True)

    # è·å–æ’åºåçš„MP4æ–‡ä»¶åˆ—è¡¨
    video_files = get_sorted_files(input_folder, 'mp4')

    # ç”»å¸ƒå¤§å° (å®½, é«˜)
    canvas_size = (1920, 1080)

    # bboxä½ç½®å’Œå¤§å° (x, y), (å®½, é«˜)
    bbox_position = (850, 100)  # ä¸­å¿ƒä½ç½®ç¤ºä¾‹
    bbox_size = (200, 200)

    for i, input_video in enumerate(video_files):
        # ç”Ÿæˆåºå·ï¼Œä¿æŒå‰å¯¼é›¶
        file_num = f"{i:02d}"

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        mask_output = os.path.join(mask_output_folder, f"{file_num}_mask.mp4")
        placed_output = os.path.join(placed_output_folder, f"{file_num}_placed.mp4")

        print(f"Processing {input_video} -> {mask_output} and {placed_output}")

        # åˆ›å»ºmaskè§†é¢‘
        create_static_video(input_video, mask_output, canvas_size, bbox_position, bbox_size)

        # åˆ›å»ºplacedè§†é¢‘
        embed_video_in_bbox(input_video, placed_output, canvas_size, bbox_position, bbox_size)

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    input_folder = "Xiang_Float_After_Tomorrow_Head_SPLITED"
    mask_output_folder = "Xiang_Float_After_Tomorrow_Head_SPLITED_mask_videos"
    placed_output_folder = "Xiang_Float_After_Tomorrow_Head_SPLITED_placed_videos"

    # å¤„ç†æ‰€æœ‰è§†é¢‘
    process_all_videos(input_folder, mask_output_folder, placed_output_folder)

en_prompt = ["Golden sunlight spills through swaying leaves,   Whispering breezes dance on rippling streams.   Moss-kissed stones hum with ancient tales,   While silver dewdrops cling to emerald veils.    Cloud shadows drift across sun-warmed hills,   Crickets weave songs through twilight's thrills.   Moonbeams trace patterns on silent ponds,   Where water lilies dream their pale blonde bonds.    Autumn's brush paints the maple's sigh,   Scarlet whispers against cobalt sky.   Frost-kissed branches etch crystal lace,   As winter's breath leaves its fleeting trace.    Dawn's first light gilds the spider's thread,   Pearls of morning on gossamer spread.   Endless cycles of earth's slow turn,   Seasons waltzing at nature's stern.",
 "The wind whispers through the trees,   Light dances on the rippling stream,   Golden leaves drift in autumn's breeze,   Silent clouds paint dreams.    Moonlight spills on the quiet lake,   Stars blink in the velvet sky,   Mist rises where the willows wake,   Night breathes a soft sigh.",
 'A starless night sky stretches endlessly, Darkness blankets the earth in silent embrace, No celestial sparks pierce the velvet void, Only the wind whispers through empty space.',
 'Silence lingers in the air, Unspoken words dissolve like mist, Empty spaces between the trees, Where echoes fade into the breeze.',
 'Golden sands slip through the breeze,   Whispering leaves let moments ease.   Shimmering streams of time flow light,   Dancing sunbeams fade from sight.',
 'The wind whispers through empty branches,   Dawn light hesitates on distant hills,   A lone leaf drifts across still waters,   Morning mist veils the silent fields.',
 'Memories weave through the branches, Whispering leaves hold the past, Golden light lingers between the trees, Silent breezes carry echoes.',
 'The wind whispers through swaying leaves, Sunlight melts into golden hues, A fleeting warmth brushes the petals, Dew trembles on morning grass.',
 'Mist swirls in endless cycles, Shadows dance in timeless rings, Dewdrops tremble on bending reeds, Twilight whispers through swaying leaves.',
 'Breeze stirs the trembling leaves, Sunlight flickers through swaying branches, Shadows dance in restless patterns, A silent pulse quickens through the air.',
 "Dappled sunlight filters through the leaves, Two shadows merge beneath the swaying boughs. The creek murmurs secrets to the stones, While golden pollen drifts on summer's breath.",
 "Golden light lingers in the air,   Soft petals drift on evening's breath.",
 'The gentle breeze empties the tranquil air, Sunlight pours through the hollowed leaves.',
 'A gentle breeze whispers through the leaves, Sunlight dances on rippling waters, Petals tremble with morning dew, Silent moments weave through the trees.',
 "Winds whisper through the trees,   Moonlight lingers on the leaves.   No questions rise with morning's glow,   Only stars that softly know.",
 "Bright mountains and clear waters   Cannot match your radiance    ï¼ˆæ³¨ï¼šæ ¹æ®è¦æ±‚ï¼Œæ”¹å†™ä¸ºçº¯è‡ªç„¶æå†™ï¼Œé¿å…äººç§°ä»£è¯å’Œäººç‰©åŠ¨ä½œï¼‰    æ”¹å†™ç‰ˆæœ¬ï¼š   Sunlit peaks and crystal streams   Pale before the dawn's first gleam",
 'Golden sunlight lingers on the winding path Autumn leaves dance in whispering breezes Two shadows merge upon the cobblestones Twilight paints the horizon in fading hues',
 'The fading light lingers low, A breeze stirs the bending reeds. Shadows stretch across the silent path, As twilight deepens in the trees.',
 'A starless night sky stretches endlessly, Darkness blankets the earth in silent embrace, No celestial sparks pierce the velvet expanse, Only the void whispers to the sleeping land.',
 'Silence lingers in the air, Unspoken words dissolve like mist. Empty spaces between the trees, Where echoes fade without reply.',
 "Promises slip through the cracks of time,   Like sand through fingers, lost in the wind's rhyme.",
 "The wind whispers through empty branches, Distant stars flicker in silent hesitation, A lone leaf trembles on the water's surface, Moonlight pools upon untouched ground.",
 'Memories weave through the whispering trees, Shadows linger where sunlight once danced. The river murmurs old melodies, While autumn leaves drift in silent trance.',
 'The wind whispers through swaying leaves,   Sunlight melts into golden hues,   A fleeting touch of warmth upon the petals,   Dew trembles on morning grass.',
 'The mist swirls in endless cycles, Shadows flicker through the veiled light, Seasons turn without beginning or end, A silent dance of fog and fading glow.',
 'The breeze quickens its restless dance, Leaves tremble without rhythm, Sunlight flickers through swaying branches, A silent storm stirs the air.',
 "Do the golden rays of dawn suffice to frame love's fleeting glow?   Can the whispering leaves alone compose the ballad of the spring?",
 'Golden light lingers in the air   Petals drift where laughter once bloomed',
 'The gentle breeze empties the tranquil air',
 'A gentle breeze whispers through the leaves, Golden sunlight dances on trembling grass, Morning dew glistens on petals unfolding, Time lingers in the fragrance of blooming flowers.',
 "Winds whisper through the trees,   Moonlight lingers on the leaves.   No questions asked by the flowing stream,   No answers given by dawn's first gleam.    Shadows dance without a care,   Stars glow softly in the air.   Tomorrow's sun may rise or fade,   Tonight the night in silence laid.",
 'Bright mountains and clear streams   pale in comparison to your radiance.',
 'Golden sunlight lingers on the winding path Autumn leaves dance in the whispering breeze Two shadows merge beneath the ancient oak Twilight descends as the river flows endlessly',
 "The fading light lingers in the air, Whispering leaves hesitate in twilight's glow, A single breeze drifts through the silent trees, As shadows stretch long across the quiet earth.",
 'Distant clouds drift across the endless blue, Sunlight dances upon the rippling stream. Two leaves tremble in the autumn breeze, Their shadows merging where the waters gleam.',
 'Golden light lingers among swaying leaves.   A gentle breeze carries whispers through the trees.',
 'The gentle breeze caresses the empty air, Sunlight pours through translucent leaves, A tranquil stream reflects the boundless sky.',
 'A fleeting moment of warmth, Sunlight dancing on dewdrops, Breezes whispering through leaves.',
 'Winds whisper through the silent trees,   Moonlight spills across the sleeping earth,   Dewdrops tremble on trembling leaves,   Stars flicker in the boundless dark.    No questions linger in the quiet air,   No answers drift with the passing clouds,   Only the endless dance of night and dawn,   Flowing like an eternal river.',
 'Sunlit peaks and crystal streams,   no sight more fair than these.',
 'The golden sunlight lingers on the winding path, Two shadows merge as twilight softly falls, Autumn leaves dance upon the silent breeze, A distant river hums its endless song.',
 'The fading light lingers in the air, Shadows stretch across the silent land.',
 'The fading light lingers,   unwilling to turn.']

cn_poems = [
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´æ¼«æ­¥åœ¨é‡‘è‰²é˜³å…‰ç©¿é€çš„æ«æ—é—´ï¼Œè½å¶åœ¨ä»–è„šä¸‹æ²™æ²™ä½œå“ï¼Œä»–å¯¹ç€æ½ºæ½ºæºªæµæ­Œå”±ï¼Œè‹”çŸ³ä¸Šçš„éœ²ç éšç€ä»–çš„æ—‹å¾‹è½»è½»é¢¤åŠ¨",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨å¾®é£å¹æ‹‚çš„æºªè¾¹ï¼Œé•œç‰‡æ˜ ç€ç²¼ç²¼æ³¢å…‰ï¼Œä»–éšç€æ‘‡æ›³çš„èŠ¦è‹‡è½»å£°å“¼å”±ï¼ŒæƒŠèµ·å‡ åªç™½é¹­æ è¿‡ç¿¡ç¿ è‰²çš„æ°´é¢",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ— æ˜Ÿçš„å¤œç©ºä¸‹ä»°å¤´æ­Œå”±ï¼Œé»‘æš—æ¸©æŸ”åŒ…è£¹ç€ä»–çš„å£°éŸ³ï¼Œåªæœ‰æ—·é‡çš„é£å¶å°”ææ¥å‡ ç¼•å›å“",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨æ™¨é›¾å¼¥æ¼«çš„æ—é—´ç©ºåœ°ï¼Œæœªå”±å®Œçš„æ­Œè¯å¦‚è–„é›¾èˆ¬æ¶ˆæ•£ï¼Œæ¾é’ˆé—´æ¼ä¸‹çš„é˜³å…‰ä¸ºä»–çš„æ­Œå£°é•€ä¸Šé‡‘è¾¹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´èµ¤è„šèµ°åœ¨å¤•é˜³æŸ“çº¢çš„æ²™æ»©ä¸Šï¼Œä»–çš„æ­Œå£°éšç€æµæ²™æ»‘è½ï¼Œä¸é€€æ½®çš„æµªèŠ±ä¸€åŒæ¶ˆé€åœ¨ç²¼ç²¼æ³¢å…‰ä¸­",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç©¿è¿‡æ™¨é›¾ç¬¼ç½©çš„éº¦ç”°ï¼Œè‰å°–çš„éœ²ç éšç€ä»–çš„éŸ³è°ƒéœ‡é¢¤ï¼Œè–„é›¾ä¸ºä»–çš„æ­Œå£°è’™ä¸Šæœ¦èƒ§é¢çº±",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´èƒŒé ç™¾å¹´è€æ ‘æ­Œå”±ï¼Œé£˜è½çš„é»„å¶å¦‚é‡‘è‰²éŸ³ç¬¦ç¯ç»•ï¼Œæ ‘çš®ç²—ç³™çš„çº¹ç†è®°å½•ç€ä»–æ­Œå£°çš„èµ·ä¼",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨æ²¾æ»¡æ™¨éœ²çš„è‹œè“¿åœ°ï¼Œä»–å‘¼å‡ºçš„ç™½æ°”ä¸æ­Œå£°èå…¥è‰å°–é¢¤æŠ–çš„éœ²ç ï¼Œè£¤ç®¡è¢«éœ²æ°´æµ¸é€ä»ä¸åœæ­Œå”±",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æš®è‰²å››åˆçš„å±±è°·æ­Œå”±ï¼Œç´«ç°è‰²çš„æš®éœ­éšç€ä»–çš„æ—‹å¾‹æµè½¬ï¼Œå¤œé£å°†æ­Œå£°é€å¾€è¿œæ–¹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨ç™½æ¡¦æ—é—´æ­Œå”±ï¼Œé˜³å…‰é€è¿‡æ ‘éš™åœ¨ä»–èº«ä¸ŠæŠ•ä¸‹å…‰æ–‘ï¼Œé£˜è½çš„æ ‘çš®åƒè¤ªè‰²çš„ä¹è°±åœ¨ä»–è„šè¾¹ç›˜æ—‹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ååœ¨æºªè¾¹åœ†çŸ³ä¸Šæ­Œå”±ï¼Œæ°´æµå†²åˆ·çŸ³å¤´çš„å£°å“ä¸ºä»–ä¼´å¥ï¼Œå‡ ç‰‡æ«å¶è½½ç€ä»–çš„éŸ³ç¬¦é¡ºæµè€Œä¸‹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨é‡èŠ±ä¸›ä¸­æ­Œå”±ï¼ŒèŠ±ç²‰æ²¾æ»¡è¡£è¢–ï¼Œæ™šé£å°†æ­Œå£°æ‰ç¢åœ¨æ‘‡æ›³çš„èŠ±å½±é‡Œ",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´æ¼«æ­¥åœ¨ç©ºå¯‚çš„æ¾æ—æ­Œå”±ï¼Œæ¾é’ˆåœ¨è„šä¸‹æ²™æ²™ä½œå“ï¼Œç©¿è¿‡æ ‘å† çš„å…‰æŸ±å¦‚å¤©ç„¶èˆå°ç¯å…‰",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´è¿ç€æ™¨é£æ­Œå”±ï¼Œéœ²æ°´æ‰“æ¹¿çƒé‹ï¼Œè‰å¶ä¸Šçš„éœ²ç éšç€éŸ³è°ƒè½»è½»éœ‡é¢¤",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ˜Ÿå…‰ä¸‹è‡ªå¼¹è‡ªå”±ï¼Œè¤ç«è™«ç”»å‡ºæµåŠ¨å…‰å¼§ï¼Œæœˆäº®åœ¨äº‘å±‚åé™é™è†å¬",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´é¢å¯¹é›ªå±±æ¹–æ³Šæ”¾æ­Œï¼Œå±±é£å·èµ·è¡£æ‘†ï¼Œå›å£°åœ¨ç¢§è“æ¹–é¢è¡èµ·ç»†çº¹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨é“ºæ»¡è½å¶çš„å°å¾„æ­Œå”±ï¼Œè„šæ­¥æ¿€èµ·é‡‘çº¢å¶æµªï¼Œè„šæ­¥å£°æ‰“ç€è‡ªç„¶èŠ‚æ‹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æš®è‰²ä¸­ä½å”±ï¼Œæ‹‰é•¿çš„å½±å­ä¸èŠ¦è‹‡ä¸›èä¸ºä¸€ä½“ï¼Œå½’å·¢é¸Ÿé›€å¶å°”åº”å’Œ",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ¼†é»‘æ—·é‡æ­Œå”±ï¼Œå£°éŸ³è¢«é»‘å¤œæ¸©æŸ”åŒ…è£¹ï¼Œè¿œå¤„ä¼ æ¥å¤œæ­çš„å›åº”",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨å¯‚é™æ£®æ—æ­Œå”±ï¼Œæ¾æ¶›å£°ä¸ºä»–å’Œå£°ï¼Œé£å¸¦èµ°å›å£°åœ¨æ ‘å† é—´æµè½¬",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´å¯¹ç€æµé€æ—¶å…‰æ­Œå”±ï¼Œé˜³å…‰å¦‚æ²™æ¼ä»æŒ‡ç¼æºœèµ°ï¼Œæ­Œå£°è¯•å›¾ç•™ä½æŒ‡é—´æ²™",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æœˆå¤œæ­Œå”±ï¼Œè½å¶åœ¨æ°´é¢å†™éŸ³ç¬¦ï¼Œå€’å½±åœ¨æ¶Ÿæ¼ªä¸­æ—¶éšæ—¶ç°",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´æ²¿è®°å¿†ä¹‹æ²³æ­Œå”±ï¼Œè€æ ‘å¹´è½®è®°å½•æ—‹å¾‹ï¼Œæ¯æåœ¨è„šä¸‹æ¸…è„†æ–­è£‚",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ™¨å…‰ä¸­æ­Œå”±ï¼Œéœ²ç æŠ˜å°„ä¸ƒå½©å…‰æ™•ï¼Œå‘¼å¸åœ¨å†·ç©ºæ°”å‡æˆç™½é›¾",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨å±±å·…æ­Œå”±ï¼Œäº‘æµ·åœ¨è„šä¸‹ç¿»æ¶Œï¼Œæœé˜³ç»™ä»–çš„è½®å»“é•€ä¸Šé‡‘è¾¹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æš´é›¨å‰æ­Œå”±ï¼Œé£æ‰¯ç€è¡£è¥Ÿï¼Œè¿œå¤„é›·å£°æ‰“ç€ä½æ²‰èŠ‚æ‹",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨éº¦æµªä¸­æ­Œå”±ï¼Œéº¦ç©—è½»æ‰«æŒå¿ƒï¼Œè‰é¸£ä¸ºä»–å’Œå£°",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨å†°æ¹–ç•”æ­Œå”±ï¼Œå‘¼å‡ºçš„ç™½æ°”å‡ç»“ï¼Œå†°å±‚ä¸‹ä¼ æ¥æ¹–æ°´éšç§˜å›å“",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨é“¶æé“æ­Œå”±ï¼Œé‡‘é»„è½å¶å¦‚é›¨ç‚¹è½ä¸‹ï¼Œé“ºå°±é‡‘è‰²åœ°æ¯¯",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨æ‚¬å´–æ­Œå”±ï¼Œæµ·é£å¸¦ç€å’¸æ¹¿æ°”æ¯ï¼ŒæµªèŠ±åœ¨å´–åº•ç¢æˆç™½è‰²éŸ³ç¬¦",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨ç«¹æ—æ­Œå”±ï¼Œæ–°ç¬‹ç ´åœŸåº”å’ŒèŠ‚å¥ï¼Œç«¹å¶æ²™æ²™å¦‚è§‚ä¼—æŒå£°",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨å‘æ—¥è‘µç”°æ­Œå”±ï¼ŒèŠ±ç›˜éšæ—‹å¾‹æ‘†åŠ¨ï¼Œèœœèœ‚å—¡å—¡ç¼–ç»‡å’Œå¼¦",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ«çº¢å±±è°·æ­Œå”±ï¼Œå›å£°åœ¨å³­å£ç¢°æ’ï¼Œé£˜è½æ«å¶å¦‚è·³åŠ¨ç«ç„°",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨é›ªåŸæ­Œå”±ï¼Œè„šå°ç»µå»¶æˆäº”çº¿è°±ï¼Œå‘¼å‡ºçš„ç™½æ°”åœ¨é˜³å…‰ä¸‹é—ªçƒ",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨è–°è¡£è‰ç”°æ­Œå”±ï¼Œç´«è‰²æ³¢æµªèµ·ä¼ï¼ŒèŠ¬èŠ³éšæ­Œå£°é£˜è¿œ",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨ç€‘å¸ƒè¾¹æ­Œå”±ï¼Œæ°´é›¾æ‰“æ¹¿ç«æ¯›ï¼Œå½©è™¹åœ¨æ°´å¹•é—´æ—¶éšæ—¶ç°",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ¨±èŠ±é›¨ä¸­æ­Œå”±ï¼Œç²‰ç™½èŠ±ç“£è½æ»¡è‚©å¤´ï¼Œè„šä¸‹è½è‹±å¦‚æŸ”è½¯ç»’æ¯¯",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨æ²™æ¼ ç»¿æ´²æ­Œå”±ï¼Œè¿œå¤„é©¼é“ƒéšçº¦ï¼Œæ£•æ¦ˆæ ‘å½±ä¸ºä»–é®é˜³",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨èŠ¦è‹‡è¡æ­Œå”±ï¼ŒèŠ¦èŠ±å¦‚é›ªç‰‡é£èˆï¼Œé‡é¸­æ‰‘ç¿…ä¸ºä»–ä¼´å¥",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨è‘¡è„æ¶ä¸‹æ­Œå”±ï¼Œè—¤è”“é—´å…‰æ–‘æ¸¸èµ°ï¼Œç†Ÿé€è‘¡è„æ•£å‘ç”œé¦™",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨ç»“éœœè‰åŸæ­Œå”±ï¼Œè„šæ­¥ç•™ä¸‹è„†å“ï¼Œå‘¼å‡ºçš„ç™½æ°”å¦‚é£˜æ•£éŸ³ç¬¦",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´ç«™åœ¨æ¶¨æ½®æµ·æ»©æ­Œå”±ï¼ŒæµªèŠ±äº²å»è„šå°–ï¼Œæµ·é¸¥é¸£å«åˆ’å‡ºå¼§çº¿",
    "ä¸€ä¸ªæˆ´çœ¼é•œç”·é’å¹´åœ¨æ™¨éœ²æœªæ™çš„è‰åŸæ­Œå”±ï¼Œè››ç½‘ä¸Šçš„éœ²ç æŠ˜å°„æ™¨å…‰ï¼Œå¦‚å¤©ç„¶èˆå°ç¯å…‰"
]

len(en_prompt), len(cn_poems)

import pandas as pd
from datasets import Dataset
Dataset.from_pandas(pd.DataFrame(zip(en_prompt, cn_poems), columns = ["en", "zh"])).push_to_hub(
    "svjack/Xiang_Float_After_Tomorrow_SPLITED_EN_ZH_Caption"
)

#vim run_outpainting.py

import os
import subprocess
import pandas as pd
from datasets import load_dataset
from moviepy.editor import VideoFileClip
from pathlib import Path

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("svjack/Xiang_Float_After_Tomorrow_SPLITED_EN_ZH_Caption")

# è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„
mask_output_folder = "Xiang_Float_After_Tomorrow_Head_SPLITED_mask_videos"
placed_output_folder = "Xiang_Float_After_Tomorrow_Head_SPLITED_placed_videos"

# è·å–å¹¶æ’åºMP4æ–‡ä»¶
def get_sorted_mp4_files(directory, extension = "mp4"):
    """Return sorted list of files with given extension in directory"""
    try:
        files = sorted(Path(directory).glob(f'*.{extension}'))
        return [str(f) for f in files]
    except Exception as e:
        print(f"Error reading {directory}: {e}")
        return []

mask_files = get_sorted_mp4_files(mask_output_folder)
placed_files = get_sorted_mp4_files(placed_output_folder)

print(len(mask_files), len(placed_files))

# ç¡®ä¿æ–‡ä»¶æ•°é‡åŒ¹é…
assert len(mask_files) == len(placed_files), "Mask and placed video counts don't match"
assert len(mask_files) == len(dataset['train']), "Video counts don't match dataset size"

# è¾…åŠ©å‡½æ•°ï¼šè·å–è§†é¢‘æ—¶é•¿
def get_video_duration(video_path):
    """ä½¿ç”¨moviepyè·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
    try:
        with VideoFileClip(video_path) as video:
            return video.duration
    except Exception as e:
        print(f"è·å–è§†é¢‘æ—¶é•¿å¤±è´¥: {video_path}, é”™è¯¯: {str(e)}")
        return 0

# è¾…åŠ©å‡½æ•°ï¼šè¿è¡Œå‘½ä»¤å¹¶è®°å½•æ—¥å¿—
def run_command(cmd, log_file=None):
    """è¿è¡Œå‘½ä»¤å¹¶æ•è·è¾“å‡ºæ—¥å¿—"""
    print(f"æ‰§è¡Œå‘½ä»¤: {cmd}")

    if log_file:
        with open(log_file, 'a') as f:
            process = subprocess.Popen(cmd, shell=True,
                                     stdout=subprocess.PIPE,
                                     stderr=subprocess.STDOUT,
                                     universal_newlines=True)

            for line in process.stdout:
                print(line.strip())  # æ‰“å°åˆ°æ§åˆ¶å°
                f.write(line)        # å†™å…¥æ—¥å¿—æ–‡ä»¶

            process.wait()
            return process.returncode
    else:
        result = subprocess.run(cmd, shell=True,
                              stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE,
                              universal_newlines=True)
        print(result.stdout)
        if result.stderr:
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
        return result.returncode

# åˆ›å»ºç»“æœè®°å½•
results = []
processed_files = []
unprocessed_files = []

# åˆ›å»ºæ—¥å¿—ç›®å½•
os.makedirs("processing_logs", exist_ok=True)

for idx, (mask_file, placed_file, zh_caption) in enumerate(zip(mask_files, placed_files, dataset['train']['zh'])):
    # è·å–è§†é¢‘è·¯å¾„å’Œæ—¶é•¿
    #video_path = os.path.join(placed_output_folder, placed_file)
    duration = get_video_duration(placed_file)

    log_file = f"processing_logs/process_{idx}_{os.path.splitext(placed_file)[0]}.log"

    if duration <= 10:
        # æ„å»ºå‘½ä»¤
        cmd = f'python vace/vace_wan_inference.py --model_name vace-1.3B ' \
              f'--src_video "{placed_file}" ' \
              f'--src_mask "{mask_file}" ' \
              f'--prompt "{zh_caption}"'

        # æ‰§è¡Œå‘½ä»¤å¹¶è®°å½•æ—¥å¿—
        return_code = run_command(cmd, log_file = None)

        results.append({
            'index': idx,
            'mask_file': mask_file,
            'placed_file': placed_file,
            'zh_caption': zh_caption,
            'processed': True,
            'duration': duration,
            'return_code': return_code,
            'log_file': log_file
        })
        processed_files.append(placed_file)
    else:
        results.append({
            'index': idx,
            'mask_file': mask_file,
            'placed_file': placed_file,
            'zh_caption': zh_caption,
            'processed': False,
            'duration': duration,
            'return_code': None,
            'log_file': None
        })
        unprocessed_files.append(placed_file)

# ä¿å­˜ç»“æœåˆ°CSV
df = pd.DataFrame(results)
df.to_csv('video_processing_report.csv', index=False)

print(f"\nå¤„ç†å®Œæˆã€‚å…±å¤„ç†äº† {len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡äº† {len(unprocessed_files)} ä¸ªæ–‡ä»¶ã€‚")
print(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ° video_processing_report.csv")
print(f"è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨ processing_logs ç›®å½•")
```



## Introduction
<strong>VACE</strong> is an all-in-one model designed for video creation and editing. It encompasses various tasks, including reference-to-video generation (<strong>R2V</strong>), video-to-video editing (<strong>V2V</strong>), and masked video-to-video editing (<strong>MV2V</strong>), allowing users to compose these tasks freely. This functionality enables users to explore diverse possibilities and streamlines their workflows effectively, offering a range of capabilities, such as Move-Anything, Swap-Anything, Reference-Anything, Expand-Anything, Animate-Anything, and more.

<img src='./assets/materials/teaser.jpg'>


## ğŸ‰ News
- [x] May 14, 2025: ğŸ”¥Wan2.1-VACE-1.3B and Wan2.1-VACE-14B models are now available at [HuggingFace](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) and [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B)!
- [x] Mar 31, 2025: ğŸ”¥VACE-Wan2.1-1.3B-Preview and VACE-LTX-Video-0.9 models are now available at [HuggingFace](https://huggingface.co/collections/ali-vilab/vace-67eca186ff3e3564726aff38) and [ModelScope](https://modelscope.cn/collections/VACE-8fa5fcfd386e43)!
- [x] Mar 31, 2025: ğŸ”¥Release code of model inference, preprocessing, and gradio demos. 
- [x] Mar 11, 2025: We propose [VACE](https://ali-vilab.github.io/VACE-Page/), an all-in-one model for video creation and editing.


## ğŸª„ Models
| Models                   | Download Link                                                                                                                                           | Video Size        | License                                                                                       |
|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-----------------------------------------------------------------------------------------------|
| VACE-Wan2.1-1.3B-Preview | [Huggingface](https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview) ğŸ¤—  [ModelScope](https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview) ğŸ¤– | ~ 81 x 480 x 832  | [Apache-2.0](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/LICENSE.txt)             |
| VACE-LTX-Video-0.9       | [Huggingface](https://huggingface.co/ali-vilab/VACE-LTX-Video-0.9) ğŸ¤—     [ModelScope](https://modelscope.cn/models/iic/VACE-LTX-Video-0.9) ğŸ¤–          | ~ 97 x 512 x 768  | [RAIL-M](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.license.txt) |
| Wan2.1-VACE-1.3B         | [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) ğŸ¤—     [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B) ğŸ¤–          | ~ 81 x 480 x 832  | [Apache-2.0](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/blob/main/LICENSE.txt)             |
| Wan2.1-VACE-14B          | [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) ğŸ¤—     [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B) ğŸ¤–            | ~ 81 x 720 x 1280 | [Apache-2.0](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B/blob/main/LICENSE.txt)             |

- The input supports any resolution, but to achieve optimal results, the video size should fall within a specific range.
- All models inherit the license of the original model.


## âš™ï¸ Installation
The codebase was tested with Python 3.10.13, CUDA version 12.4, and PyTorch >= 2.5.1.

### Setup for Model Inference
You can setup for VACE model inference by running:
```bash
git clone https://github.com/ali-vilab/VACE.git && cd VACE
pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu124  # If PyTorch is not installed.
pip install -r requirements.txt
pip install wan@git+https://github.com/Wan-Video/Wan2.1  # If you want to use Wan2.1-based VACE.
pip install ltx-video@git+https://github.com/Lightricks/LTX-Video@ltx-video-0.9.1 sentencepiece --no-deps # If you want to use LTX-Video-0.9-based VACE. It may conflict with Wan.
```
Please download your preferred base model to `<repo-root>/models/`. 

### Setup for Preprocess Tools
If you need preprocessing tools, please install:
```bash
pip install -r requirements/annotator.txt
```
Please download [VACE-Annotators](https://huggingface.co/ali-vilab/VACE-Annotators) to `<repo-root>/models/`.

### Local Directories Setup
It is recommended to download [VACE-Benchmark](https://huggingface.co/datasets/ali-vilab/VACE-Benchmark) to `<repo-root>/benchmarks/` as examples in `run_vace_xxx.sh`.

We recommend to organize local directories as:
```angular2html
VACE
â”œâ”€â”€ ...
â”œâ”€â”€ benchmarks
â”‚   â””â”€â”€ VACE-Benchmark
â”‚       â””â”€â”€ assets
â”‚           â””â”€â”€ examples
â”‚               â”œâ”€â”€ animate_anything
â”‚               â”‚   â””â”€â”€ ...
â”‚               â””â”€â”€ ...
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ VACE-Annotators
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ VACE-LTX-Video-0.9
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ VACE-Wan2.1-1.3B-Preview
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
```

## ğŸš€ Usage
In VACE, users can input **text prompt** and optional **video**, **mask**, and **image** for video generation or editing.
Detailed instructions for using VACE can be found in the [User Guide](./UserGuide.md).

### Inference CIL
#### 1) End-to-End Running
To simply run VACE without diving into any implementation details, we suggest an end-to-end pipeline. For example:
```bash
# run V2V depth
python vace/vace_pipeline.py --base wan --task depth --video assets/videos/test.mp4 --prompt 'xxx'

# run MV2V inpainting by providing bbox
python vace/vace_pipeline.py --base wan --task inpainting --mode bbox --bbox 50,50,550,700 --video assets/videos/test.mp4 --prompt 'xxx'
```
This script will run video preprocessing and model inference sequentially, 
and you need to specify all the required args of preprocessing (`--task`, `--mode`, `--bbox`, `--video`, etc.) and inference (`--prompt`, etc.). 
The output video together with intermediate video, mask and images will be saved into `./results/` by default.

> ğŸ’¡**Note**:
> Please refer to [run_vace_pipeline.sh](./run_vace_pipeline.sh) for usage examples of different task pipelines.


#### 2) Preprocessing
To have more flexible control over the input, before VACE model inference, user inputs need to be preprocessed into `src_video`, `src_mask`, and `src_ref_images` first.
We assign each [preprocessor](./vace/configs/__init__.py) a task name, so simply call [`vace_preprocess.py`](./vace/vace_preproccess.py) and specify the task name and task params. For example:
```angular2html
# process video depth
python vace/vace_preproccess.py --task depth --video assets/videos/test.mp4

# process video inpainting by providing bbox
python vace/vace_preproccess.py --task inpainting --mode bbox --bbox 50,50,550,700 --video assets/videos/test.mp4
```
The outputs will be saved to `./processed/` by default.

> ğŸ’¡**Note**:
> Please refer to [run_vace_pipeline.sh](./run_vace_pipeline.sh) preprocessing methods for different tasks.
Moreover, refer to [vace/configs/](./vace/configs/) for all the pre-defined tasks and required params.
You can also customize preprocessors by implementing at [`annotators`](./vace/annotators/__init__.py) and register them at [`configs`](./vace/configs).


#### 3) Model inference
Using the input data obtained from **Preprocessing**, the model inference process can be performed as follows:
```bash
# For Wan2.1 single GPU inference (1.3B-480P)
python vace/vace_wan_inference.py --ckpt_dir <path-to-model> --src_video <path-to-src-video> --src_mask <path-to-src-mask> --src_ref_images <paths-to-src-ref-images> --prompt "xxx"

# For Wan2.1 Multi GPU Acceleration inference (1.3B-480P)
pip install "xfuser>=0.4.1"
torchrun --nproc_per_node=8 vace/vace_wan_inference.py --dit_fsdp --t5_fsdp --ulysses_size 1 --ring_size 8 --ckpt_dir <path-to-model> --src_video <path-to-src-video> --src_mask <path-to-src-mask> --src_ref_images <paths-to-src-ref-images> --prompt "xxx"

# For Wan2.1 Multi GPU Acceleration inference (14B-720P)
torchrun --nproc_per_node=8 vace/vace_wan_inference.py --dit_fsdp --t5_fsdp --ulysses_size 8 --ring_size 1 --size 720p --model_name 'vace-14B' --ckpt_dir <path-to-model> --src_video <path-to-src-video> --src_mask <path-to-src-mask> --src_ref_images <paths-to-src-ref-images> --prompt "xxx"

# For LTX inference, run
python vace/vace_ltx_inference.py --ckpt_path <path-to-model> --text_encoder_path <path-to-model> --src_video <path-to-src-video> --src_mask <path-to-src-mask> --src_ref_images <paths-to-src-ref-images> --prompt "xxx"
```
The output video together with intermediate video, mask and images will be saved into `./results/` by default.

> ğŸ’¡**Note**: 
> (1) Please refer to [vace/vace_wan_inference.py](./vace/vace_wan_inference.py) and [vace/vace_ltx_inference.py](./vace/vace_ltx_inference.py) for the inference args.
> (2) For LTX-Video and English language Wan2.1 users, you need prompt extension to unlock the full model performance. 
Please follow the [instruction of Wan2.1](https://github.com/Wan-Video/Wan2.1?tab=readme-ov-file#2-using-prompt-extension) and set `--use_prompt_extend` while running inference.
> (3) When performing prompt extension in editing tasks, it's important to pay attention to the results of expanding plain text. Since the visual information being input is unknown, this may lead to the extended output not matching the video being edited, which can affect the final outcome.

### Inference Gradio
For preprocessors, run 
```bash
python vace/gradios/vace_preprocess_demo.py
```
For model inference, run
```bash
# For Wan2.1 gradio inference
python vace/gradios/vace_wan_demo.py

# For LTX gradio inference
python vace/gradios/vace_ltx_demo.py
```

## Acknowledgement

We are grateful for the following awesome projects, including [Scepter](https://github.com/modelscope/scepter), [Wan](https://github.com/Wan-Video/Wan2.1), and [LTX-Video](https://github.com/Lightricks/LTX-Video).


## BibTeX

```bibtex
@article{vace,
    title = {VACE: All-in-One Video Creation and Editing},
    author = {Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},
    journal = {arXiv preprint arXiv:2503.07598},
    year = {2025}
}
